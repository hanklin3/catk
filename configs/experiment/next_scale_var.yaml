# @package _global_

defaults:
  # - override /trainer: ddp
  - override /model: smart

model:
  model_config:
    # lr: 5e-4
    lr: 1e-4
    lr_min_ratio: 1e-2
    token_processor:
      map_token_sampling: # open-loop
        num_k: 1 # for k nearest neighbors
        temp: 1.0 # uniform sampling
      agent_token_sampling: # closed-loop
        num_k: 1 # for k nearest neighbors
        temp: 1.0
    gradient_clip_val: 5e-2
    gradient_clip_algorithm: "norm" # "value" or "norm"
    # overflow_new_scale: 32768.0 # fp16 overflows >65536.0
    overflow_new_scale: 65536.0 # 

    training_rollout_sampling:
      criterium: next_scale_var # {topk_dist_sampled_with_prob, topk_prob, topk_prob_sampled_with_dist}
      num_k: 1 # for k nearest neighbors, set to -1 to turn-off closed-loop training
      temp: 1e-5 # catk = topk_prob_sampled_with_dist with temp=1e-5
      n_points_per_level: [1, 2, 3, 5, 9, 16, 18]
      using_znorm_cos_sim_ow_L2: false

    decoder:
      build_vqvae: true
      build_var: true
      n_vq_emb: 32  # vqvae quantized vector emb size
      vq_vocab_size: 4096 # vqvae quantized vector latent vocab size
      var_precision: "bfloat16" # float16 or bfloat16 or null (32 bit)
      finetune_vqvae: true # if build_var is true, finetune_vqvae. (SMART before vae is still frozen).


# ckpt_path: logs/pre_bc-debug/runs/2024-12-18_19-17-33/checkpoints/epoch_009.ckpt #null
# ckpt_path: CKPT_FOR_RESUME.ckpt # to resume training
# ckpt_path: logs/pre_bc-debug/runs/2024-12-19_10-04-48/checkpoints/last.ckpt
# ckpt_path: logs/next_scale-0003_vqvae_loss_using_logits__loss_sum_fix/runs/2025-01-21_09-45-24/checkpoints/epoch_063.ckpt
# ckpt_path: logs/next_scale-0004_vocabSize4096_Cvae64_vqvae_varClass/runs/2025-02-03_23-38-25/checkpoints/last.ckpt
# ckph_path: logs/next_scale-0004_vqvae_varClass_rectangle/runs/2025-02-04_14-39-28/checkpoints/last.ckpt
ckpt_path: logs/next_scale-0004_vqvae_varClass_rectangle_Cvae32/runs/2025-02-05_23-38-22/checkpoints/last.ckpt
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-11_20-35-55/checkpoints/epoch_000.ckpt

## same model resume training
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-11_20-50-42/checkpoints/epoch_000.ckpt
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-11_21-51-54/checkpoints/epoch_001.ckpt
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-11_22-49-52/checkpoints/epoch_003.ckpt
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-12_00-32-29/checkpoints/epoch_004.ckpt
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-12_01-31-27/checkpoints/epoch_005.ckpt
# ckpt_path: logs/next_scale_var-0005_VAR_32-true_fullvqvae/runs/2025-02-12_08-26-38/checkpoints/epoch_006.ckpt

trainer:
  limit_train_batches: 1.0
  limit_val_batches: 0.1
  check_val_every_n_epoch: 1
  max_epochs: 64
  # gradient_clip_val: 1e-3
  # gradient_clip_val: 0.0001
  # gradient_clip_val: 0.00005
  # gradient_clip_val: 0.00001
  # gradient_clip_val: 5.0e-06 #0.000005
  # gradient_clip_val: 1.0e-06
  # gradient_clip_val: 1.0e-07
  # gradient_clip_val: 2.0
  # gradient_clip_val: 0.05
  # gradient_clip_algorithm: "value"
  detect_anomaly: true

data:
  train_batch_size: 2 #10
  val_batch_size: 4 #10
  test_batch_size: 4 #10
  num_workers: 10

action: finetune  # if only load vqvae
# action: fit # resume previous failed training