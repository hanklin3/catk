# @package _global_

defaults:
  # - override /trainer: ddp
  - override /model: smart

model:
  model_config:
    lr: 5e-4
    lr_min_ratio: 1e-2
    token_processor:
      map_token_sampling: # open-loop
        num_k: 1 # for k nearest neighbors
        temp: 1.0 # uniform sampling
      agent_token_sampling: # closed-loop
        num_k: 1 # for k nearest neighbors
        temp: 1.0

    training_rollout_sampling:
      criterium: next_scale # {topk_dist_sampled_with_prob, topk_prob, topk_prob_sampled_with_dist}
      num_k: 1 # for k nearest neighbors, set to -1 to turn-off closed-loop training
      temp: 1e-5 # catk = topk_prob_sampled_with_dist with temp=1e-5
      n_points_per_level: [1, 2, 3, 5, 9, 16, 18]
      using_znorm_cos_sim_ow_L2: false

    decoder:
      build_vqvae: true
      build_var: false
      n_vq_emb: 32  # vqvae quantized vector emb size
      vq_vocab_size: 4096 # vqvae quantized vector latent vocab size
      var_precision: "bfloat16" # float16 or bfloat16 or null (32 bit)
      finetune_vqvae: true # if build_var is true, finetune_vqvae. (SMART before vae is still frozen).

# ckpt_path: logs/pre_bc-debug/runs/2024-12-18_19-17-33/checkpoints/epoch_009.ckpt #null
# ckpt_path: CKPT_FOR_RESUME.ckpt # to resume training
# ckpt_path: logs/pre_bc-debug/runs/2024-12-19_10-04-48/checkpoints/last.ckpt
# ckpt_path: logs/next_scale-vqvae_loss_vqvae_logits_gradFix/runs/2025-01-20_23-31-42/checkpoints/epoch_008.ckpt
# ckpt_path: logs/next_scale-0004_vqvae_varClass_rectangle_Cvae32/runs/2025-02-05_11-30-16/checkpoints/last.ckpt
# ckpt_path: logs/next_scale-0004_vqvae_varClass_rectangle_Cvae32/runs/2025-02-05_15-33-32/checkpoints/epoch_005.ckpt

trainer:
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  check_val_every_n_epoch: 10
  max_epochs: 64

data:
  train_batch_size: 2 #10
  val_batch_size: 4 #10
  test_batch_size: 4 #10
  num_workers: 10

action: fit #finetune